{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245e80ee",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00db346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "!pip install mlxtend\n",
    "!pip install scikit-learn-extra\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score , classification_report, ConfusionMatrixDisplay,precision_score,recall_score, f1_score,roc_auc_score,roc_curve, balanced_accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn_extra.cluster import KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataset\n",
    "data = pd.read_csv(\"data/stroke_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492f96e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640f7de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ee473",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 3 rows that are missing a value in the \"sex\" column - because of the negligible count, we will just drop it\n",
    "# Drop rows with NaN values in the 'sex' column\n",
    "data = data.dropna(subset=[\"sex\"])\n",
    "\n",
    "# Reset index after dropping rows\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a914b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are NA values.\n",
    "data.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are duplicate values.\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c096c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, there is no missing/duplicate data.\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ba57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To analyze the data in a better way, we decided to convert the coding to the real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5905e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_vis = data.copy()\n",
    "data_vis.sex[data_vis['sex'] == 0] = 'Female'\n",
    "data_vis.sex[data_vis['sex'] == 1] = 'Male'\n",
    "\n",
    "data_vis.hypertension[data_vis['hypertension'] == 0] = 'Not had hypertension'\n",
    "data_vis.hypertension[data_vis['hypertension'] == 1] = 'Had hypertension'\n",
    "\n",
    "data_vis.heart_disease[data_vis['heart_disease'] == 0] = 'Not had heart disease'\n",
    "data_vis.heart_disease[data_vis['heart_disease'] == 1] = 'Had heart disease'\n",
    "\n",
    "data_vis.ever_married[data_vis['ever_married'] == 0] = 'No'\n",
    "data_vis.ever_married[data_vis['ever_married'] == 1] = 'Yes'\n",
    "\n",
    "data_vis.work_type[data_vis['work_type'] == 0] = 'Never worked'\n",
    "data_vis.work_type[data_vis['work_type'] == 1] = 'Children'\n",
    "data_vis.work_type[data_vis['work_type'] == 2] = 'Govt job'\n",
    "data_vis.work_type[data_vis['work_type'] == 3] = 'Self-employed'\n",
    "data_vis.work_type[data_vis['work_type'] == 4] = 'Private'\n",
    "\n",
    "data_vis.Residence_type[data_vis['Residence_type'] == 0] = 'Urban'\n",
    "data_vis.Residence_type[data_vis['Residence_type'] == 1] = 'Rural'\n",
    "\n",
    "data_vis.smoking_status[data_vis['smoking_status'] == 0] = 'Never smoked'\n",
    "data_vis.smoking_status[data_vis['smoking_status'] == 1] = 'Smokes'\n",
    "\n",
    "data_vis.stroke[data_vis['stroke'] == 0] = 'No'\n",
    "data_vis.stroke[data_vis['stroke'] == 1] = 'Yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb43287",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "for col in data_vis.columns:\n",
    "    unique_values[col] = data_vis[col].value_counts().shape[0]\n",
    "\n",
    "pd.DataFrame(unique_values, index=['unique value count']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"work_type\" column will need One Hot Encoder to process in nominal categorical data.\n",
    "# \"age\", \"avg_glucose_level\" and \"bmi\" columns can be processed using Binning or Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of the value of the features in the dataset\n",
    "feature_cols = [x for x in data_vis.columns if x not in 'stroke']\n",
    "plt.figure(figsize=(25,35))\n",
    "# loop for subplots\n",
    "for i in range(len(feature_cols)):\n",
    "    plt.subplot(8,5,i+1)\n",
    "    plt.title(feature_cols[i])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.hist(data_vis[feature_cols[i]],color = \"deepskyblue\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There can't be a negative age, so lets drop these rows\n",
    "data = data.drop(data[data.age < 0].index)\n",
    "data_vis = data_vis.drop(data_vis[data_vis.age < 0].index)\n",
    "len(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04062bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We dropped 58 rows\n",
    "# Traget values frequency\n",
    "plt.figure(figsize=(8,6))\n",
    "labels = ['Stroke', 'No-Stroke']\n",
    "sizes = [data_vis['stroke'].value_counts()['Yes'],data_vis['stroke'].value_counts()['No']]\n",
    "colors = ['crimson', 'deepskyblue']\n",
    "explode = (0.01,0.01)  # explode 1st slice\n",
    "plt.pie(sizes, explode=explode, labels=labels, autopct='%.1f%%', colors=colors, data = data_vis);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction column classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f637e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between each feature and stroke\n",
    "data.drop('stroke', axis=1).corrwith(data.stroke).plot(kind='bar', grid=True, figsize=(10, 6), title=\"Correlation with Stroke\",color=\"deepskyblue\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features \"bmi\", \"Residence_type\", \"work_type\" are least correlated with Storke. All other features have a significant correlation with Stroke.\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(15,10))\n",
    "mask=np.triu(data.corr())\n",
    "sns.heatmap(data.corr(),mask=mask,annot=True,cmap='Blues_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no correlation higher than +- 0.3 for a pair of features. Therefore, we cannot drop features at this time.\n",
    "\n",
    "# Stroke frequency by categorical features\n",
    "features = [x for x in data_vis.columns if x not in ['stroke','bmi','age','avg_glucose_level']]\n",
    "plt.figure(figsize = (30,23))\n",
    "plt.suptitle('Stroke by categorical features')\n",
    "#subplots\n",
    "for i in enumerate(features):\n",
    "    plt.subplot(2,4, i[0]+1)\n",
    "    x = sns.countplot(i[1] ,hue='stroke', data=data_vis, palette = ['deepskyblue','crimson'])\n",
    "    for z in x.patches:\n",
    "      x.annotate('{:.1f}'.format((z.get_height()/data_vis.shape[0])*100)+'%',(z.get_x()+0.25, z.get_height()+0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bad812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex has no effect on stroke.\n",
    "\n",
    "# Smoking, hypertension and heart disease increase the risk of stroke.\n",
    "\n",
    "# People whose type of work is \"self employee\" have a higher risk of stroke than other types of work.\n",
    "# There is no clear connection between residence type and stroke, but there is a slight tendency to stroke in a rural residence type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroke frequency by numerical features\n",
    "\n",
    "# scale the data before pairplot\n",
    "data_pairplot = data.copy()\n",
    "float_columns = [x for x in data.columns if x in ['bmi','age','avg_glucose_level']]\n",
    "\n",
    "sc = StandardScaler()\n",
    "data_pairplot[float_columns] = sc.fit_transform(data_pairplot[float_columns])\n",
    "data_pairplot.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = [x for x in data_pairplot.columns if x in ['bmi','age','avg_glucose_level']]\n",
    "sns.set_context('notebook')\n",
    "sns.pairplot(data_pairplot[float_columns + ['stroke']], \n",
    "             hue='stroke', \n",
    "             hue_order=[0,1],\n",
    "             height=3,\n",
    "             palette={0:'deepskyblue',1:'crimson'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7644988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining of float features does not seem to explain stroke.\n",
    "\n",
    "# Numeric features distributions\n",
    "# Bmi\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.displot(x='bmi', col='stroke' , data = data_vis, kind=\"kde\" ,color = 'deepskyblue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the data to see the distribution clearly\n",
    "bmi = pd.cut( data_vis['bmi'],bins=[0,18.5,25,30,35,max(data_vis['bmi'])],labels=['Underweight','Normal','Overweight','Obesity','Extremly Obesity'])\n",
    "plt.figure(figsize=(12,20))\n",
    "sns.displot(data=data_vis,col='stroke',x=bmi,color='deepskyblue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0de05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29557bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bmi's distribution of target column values are similar.\n",
    "\n",
    "# Age\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.displot(x='age', col='stroke' , data = data_vis, kind=\"kde\" ,color = 'deepskyblue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The age's distribution of target column values are similar.\n",
    "\n",
    "# avg_glucose_level\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.displot(x='avg_glucose_level', col='stroke' , data = data_vis, kind=\"kde\" ,color = 'deepskyblue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2084c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning the data to see the distribution clearly\n",
    "\n",
    "avg_glucose_level = pd.cut( data_vis['avg_glucose_level'],bins=[0,100,125,max(data_vis['avg_glucose_level'])],labels=['normal','impaired glucose','diabetic'])\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.displot(data=data_vis,col='stroke',x=avg_glucose_level,color='deepskyblue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of the average glucose level of the target column values is different. This feature is an explanatory feature; People with high blood glucose are more likely to have a stroke than those with normal values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12d708",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ed69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop values with low frequency\n",
    "# Let's look at the \"work_type\" feature graph:\n",
    "plt.figure(figsize=(8,6))\n",
    "x = sns.countplot(\"work_type\" ,hue='stroke', data=data_vis, palette = ['deepskyblue','crimson'])\n",
    "for z in x.patches:\n",
    "    x.annotate('{:.1f}'.format((z.get_height()/data_vis.shape[0])*100)+'%',(z.get_x()+0.25, z.get_height()+0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e156a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 1.3 percent of the data are divided into the values \"children\", \"never worked\". In addition, neither of these two values had a stroke. This data can cause biases, so I will drop these records.\n",
    "\n",
    "data.drop(data[data.work_type <= 1].index, inplace=True)\n",
    "plt.figure(figsize=(8,6))\n",
    "x = sns.countplot(\"work_type\" ,hue='stroke', data=data, palette = ['deepskyblue','crimson'])\n",
    "for z in x.patches:\n",
    "    x.annotate('{:.1f}'.format((z.get_height()/data.shape[0])*100)+'%',(z.get_x()+0.25, z.get_height()+0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb06212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"work_type\" feature is of the Nominal Variable type. Therefore One Hot Encoder must be performed.\n",
    "\n",
    "data.work_type[data['work_type'] == 2] = 'Govt job'\n",
    "data.work_type[data['work_type'] == 3] = 'Self-employed'\n",
    "data.work_type[data['work_type'] == 4] = 'Private'\n",
    "one_hot_encode_cols = ['work_type']  # filtering by string categoricals\n",
    "# Encode these columns as categoricals so one hot encoding works on split data \n",
    "for col in one_hot_encode_cols:\n",
    "    data[col] = pd.Categorical(data[col])\n",
    "# Do the one hot encoding\n",
    "data = pd.get_dummies(data, columns=one_hot_encode_cols)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d284a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skweness check\n",
    "data_skew = data[['age','avg_glucose_level','bmi']]\n",
    "skew = pd.DataFrame(data_skew.skew())\n",
    "skew.columns = ['skew']\n",
    "skew['too_skewed'] = skew['skew'] > .75\n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3dd2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 2 columns with high Skewness. Therefore, we will normalize them using the QuantileTransformer method.\n",
    "qt = QuantileTransformer(n_quantiles=500, output_distribution='normal')\n",
    "data[['bmi']] = qt.fit_transform(data[['bmi']])\n",
    "data[['avg_glucose_level']] = qt.fit_transform(data[['avg_glucose_level']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9db77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_skew = data[['age','avg_glucose_level','bmi']]\n",
    "skew = pd.DataFrame(data_skew.skew())\n",
    "skew.columns = ['skew']\n",
    "skew['too_skewed'] = skew['skew'] > .75\n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the columns are not skewed.\n",
    "\n",
    "# Data Scaling\n",
    "sc = StandardScaler()\n",
    "data[['bmi']] = sc.fit_transform(data[['bmi']])\n",
    "data[['age']] = sc.fit_transform(data[['age']])\n",
    "data[['avg_glucose_level']] = sc.fit_transform(data[['avg_glucose_level']])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data[['bmi','age','avg_glucose_level']]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "y = (data['stroke']).astype(int)\n",
    "X = data.loc[:, data.columns != 'stroke']  # everything except \"stroke\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that gets y_test and calculates all the relevant metric\n",
    "def train_evaluate_model(y_test):\n",
    "    #fit the model instance \n",
    "    predictions = y_pred_test # calculate predictions\n",
    "\n",
    "    #compute metrics for evaluation\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    #create a dataframe to visualize the results\n",
    "    eval_df = pd.DataFrame([[accuracy, f1, precision, recall, balanced_accuracy, auc]], columns=['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'auc'])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e72b62",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fdfaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for the best model parameters with GridSearchCV\n",
    "# defining parameter range\n",
    "param_grid = {'max_depth':range(1, dt.tree_.max_depth+1, 2),\n",
    "              'max_features': range(1, len(dt.feature_importances_)+1)}  \n",
    "gridDT = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, n_jobs=-1)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "gridDT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20744a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "print(gridDT.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78464d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction according to this model\n",
    "y_pred_test = gridDT.predict(X_test)\n",
    "y_pred_train = gridDT.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3546a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfit check\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "print(accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no overfitting, as the model is performing well on both the train data and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a68579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(35, 15))\n",
    "plot_tree(gridDT.best_estimator_, feature_names=X_train.columns, class_names=['0', '1'], filled=True, rounded=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_evaluate_model(y_test)\n",
    "results.index = ['Decision Tree']\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gridDT,X_test,y_test,cmap = \"copper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fb754",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [1,10,100,1000], \n",
    "                'gamma': [1, 0.1, 0.01, 0.001],\n",
    "                'kernel': ['rbf']} \n",
    "    \n",
    "gridSVM = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, cv=2)\n",
    "    \n",
    "# fitting the model for grid search\n",
    "gridSVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the best parameters\n",
    "print(gridSVM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba3b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction according to this model\n",
    "y_pred_test = gridSVM.predict(X_test)\n",
    "y_pred_train = gridSVM.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfit check\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "print(accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c708534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is low overfitting, but the model results are good\n",
    "resultsSVM = train_evaluate_model(y_test)\n",
    "resultsSVM.index = ['Support Vector Machine']\n",
    "results = results.append(resultsSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8999e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gridSVM,X_test,y_test,cmap = \"copper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31086fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create pair plot with decision boundaries\n",
    "def pair_plot_with_decision_boundaries(model, X, y):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Create a DataFrame from the training data\n",
    "    df = pd.DataFrame(X, columns=X_train.columns)\n",
    "    df['target'] = le.inverse_transform(y_encoded)\n",
    "\n",
    "    # Create a pair plot\n",
    "    sns.pairplot(df, hue='target', palette='viridis')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize pair plot with decision boundaries\n",
    "pair_plot_with_decision_boundaries(gridSVM.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Visualize decision boundaries in the reduced space\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Get the decision boundaries\n",
    "Z = gridSVM.best_estimator_.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "\n",
    "# Scatter plot of the reduced data points\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k', marker='o')\n",
    "plt.title('SVM Decision Boundaries after PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56e30d",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2ea5b",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means Clustering\n",
    "\n",
    "file_path = 'data/stroke_data.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data = pd.get_dummies(data, columns=['sex', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'])\n",
    "\n",
    "# Scale numerical values\n",
    "# scaler = StandardScaler()\n",
    "# numerical_cols = ['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']\n",
    "# data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Focus on stroke column for clustering\n",
    "X = data.drop('stroke', axis=1)\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)  # You can adjust the number of clusters\n",
    "data['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.scatter(data['age'], data['avg_glucose_level'], c=data['cluster'], cmap='viridis')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Glucose Level')\n",
    "plt.title('KMeans Clustering of Brain Stroke Data')\n",
    "\n",
    "# Adding legend\n",
    "plt.colorbar(label='Stroke')\n",
    "plt.legend(handles=[], labels=[\"Cluster 0\", \"Cluster 1\"]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe2d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensions for visualization (2D)\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "# Visualize clusters in 2D\n",
    "plt.scatter(principalComponents[:, 0], principalComponents[:, 1], c=data['cluster'], cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('KMeans Clustering (PCA Visualization)')\n",
    "\n",
    "# Create legend handles and labels\n",
    "# legend_labels = [f'Cluster {i}' for i in range(len(set(data[\"cluster\"])))]\n",
    "# plt.legend(handles=plt.scatter.legend_elements()[0], labels=legend_labels)\n",
    "\n",
    "plt.colorbar(label='Stroke')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation\n",
    "\n",
    "# Evaluating the performance of k-means clustering using Inertia, Silhouette Score and Davies-Bouldin Index metrics\n",
    "# Fit KMeans model\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "data['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Inertia\n",
    "inertia = kmeans.inertia_\n",
    "print(\"Inertia:\", inertia)\n",
    "# the result is quite high, which might indicate that the clusters are not tightly grouped.\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette = silhouette_score(X, data['cluster'])\n",
    "print(\"Silhouette Score:\", silhouette)\n",
    "#  A higher silhouette score indicates better-defined clusters. Since the silhoutte score is 0.645, which is is reasonable,\n",
    "#  it indicates there are well-separated clusters.\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "davies_bouldin = davies_bouldin_score(X, data['cluster'])\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin)\n",
    "# A lower score suggests better separation between clusters. Since thescore is 0.515, it indicates good cluster distinctiveness.\n",
    "\n",
    "\n",
    "# Overall, even though the inertia is relatively high, the Silhouette Score and Davies-Bouldin Index suggest decent \n",
    "# clustering performance with well-defined and distinct clusters in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b30d5b",
   "metadata": {},
   "source": [
    "## K-medoids Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32327d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-medoids clustering\n",
    "\n",
    "X = data.drop('stroke', axis=1)\n",
    "\n",
    "# Perform KMedoids clustering\n",
    "n_clusters = 2  # Specify the number of clusters\n",
    "kmedoids = KMedoids(n_clusters=n_clusters, random_state=42)\n",
    "data['cluster'] = kmedoids.fit_predict(X)\n",
    "\n",
    "# Visualizing clusters\n",
    "plt.scatter(data['age'], data['avg_glucose_level'], c=data['cluster'], cmap='viridis')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Glucose Level')\n",
    "plt.title('KMedoids Clustering of Brain Stroke Data')\n",
    "\n",
    "# Adding legend\n",
    "plt.colorbar(label='Stroke')\n",
    "plt.legend(handles=[], labels=[f\"Cluster {i}\" for i in range(n_clusters)]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74d0b8",
   "metadata": {},
   "source": [
    "# Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26176e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association Rule Mining\n",
    "\n",
    "stroke_data = pd.read_csv(\"data/stroke_data.csv\")\n",
    "stroke_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping numerical values of 'hypertension' column to categorical labels\n",
    "stroke_data['hypertension'] =stroke_data['hypertension'].map({0: 'No Hypertension', 1: 'Hypertension'})\n",
    "# brain['hypertension'] = brain['hypertension'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping numerical values of 'heart_disease' column to categorical labels\n",
    "stroke_data['heart_disease'].fillna(0, inplace=True)\n",
    "stroke_data['heart_disease'] = stroke_data['heart_disease'].map({0: 'No Heart Disease', 1: 'Heart Disease'})\n",
    "# brain['heart_disease'] = brain['heart_disease'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping numerical values of 'smoking_status' column to categorical labels\n",
    "stroke_data['smoking_status'].fillna(0, inplace=True)\n",
    "stroke_data['smoking_status'] = stroke_data['smoking_status'].map({1: 'smokes', 0: 'never smoked'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eab066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data\n",
    "stroke_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e218e",
   "metadata": {},
   "source": [
    "## Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50043741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for association\n",
    "association_data = stroke_data[['hypertension', 'heart_disease', 'smoking_status']]\n",
    "\n",
    "# Applying one-hot encoding to convert categorical variables to binary indicators\n",
    "association_data_encoded = pd.get_dummies(association_data)\n",
    "\n",
    "# Applying Apriori Algorithm\n",
    "frequent_itemsets_apriori = apriori(association_data_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Generating association rules for the given data\n",
    "rules = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=0.7)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the frequent itemsets\n",
    "frequent_itemsets_apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation\n",
    "\n",
    "#Performing metrics-based evaluation on the association rules generated by the Apriori algorithm\n",
    "# Assessing Support\n",
    "min_support_threshold = 0.1\n",
    "high_support_rules = rules[rules['support'] > min_support_threshold]\n",
    "high_support_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Confidence\n",
    "min_confidence_threshold = 0.7\n",
    "high_confidence_rules = rules[rules['confidence'] > min_confidence_threshold]\n",
    "high_confidence_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ac198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Lift\n",
    "min_lift_threshold = 1.0 \n",
    "high_lift_rules = rules[rules['lift'] > min_lift_threshold]\n",
    "high_lift_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7471bba",
   "metadata": {},
   "source": [
    "## FP Growth Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating frequent itemsets using FP-Growth algorithm \n",
    "\n",
    "# Selecting required columns for FP-Growth\n",
    "association_data = stroke_data[['hypertension', 'heart_disease', 'smoking_status']]\n",
    "\n",
    "# Applying one-hot encoding to convert categorical variables to binary indicators\n",
    "association_data_encoded = pd.get_dummies(association_data)\n",
    "\n",
    "# Applying FP-Growth algorithm\n",
    "frequent_itemsets_fpgrowth = fpgrowth(association_data_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Print the result\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation\n",
    "\n",
    "# Generate association rules from frequent itemsets\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# Evaluate the generated association rules using support, confidence, and lift metrics:\n",
    "print(\"Support:\")\n",
    "print(rules[['support']])\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"Confidence:\")\n",
    "print(rules[['confidence']])\n",
    "print(\"=\" * 40)\n",
    "# Since the confidence values are high it suggest strong associations between the antecedent and consequent\n",
    "\n",
    "print(\"Lift:\")\n",
    "print(rules[['lift']])\n",
    "print(\"=\" * 40)\n",
    "# Lift values of 1.0 suggest that there's no significant association between the antecedent and consequent.\n",
    "\n",
    "# Overall, high confidence levels and a lift of 1.0 may suggest that, despite the great degree of confidence in the rules,\n",
    "# there is not much of a dependency between the antecedents and consequents because the lift barely varies from 1.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
